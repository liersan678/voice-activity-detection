{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1c8N_gw3F_V32-Fr8UH84D3Tg4Pz4LF-h",
      "authorship_tag": "ABX9TyOFhtSTMMjvTr1vIr6mHWnx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liersan678/voice-activity-detection/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euUuHCEAeIRj",
        "outputId": "c79fc5e1-bc56-442a-b7ff-1b7b7b1a9372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install sox libsox-fmt-mp3 libsndfile1 ffmpeg\n",
        "!pip install deepl srt ffmpeg-python spleeter\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-YykCuCecF8",
        "outputId": "6e3186ba-4eba-47d4-87c7-10f31f108775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.28-7ubuntu0.1).\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following additional packages will be installed:\n",
            "  libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3\n",
            "Suggested packages:\n",
            "  libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox-fmt-mp3 libsox3 sox\n",
            "0 upgraded, 9 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 623 kB of archives.\n",
            "After this operation, 1,950 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libid3tag0 amd64 0.15.1b-14 [31.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libmad0 amd64 0.15.1b-10ubuntu1 [63.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [225 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [10.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [31.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [102 kB]\n",
            "Fetched 623 kB in 2s (402 kB/s)\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "(Reading database ... 128285 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libid3tag0_0.15.1b-14_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../1-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../2-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../3-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../4-libsox3_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../6-libsox-fmt-base_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../7-libsox-fmt-mp3_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../8-sox_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepl\n",
            "  Downloading deepl-1.14.0-py3-none-any.whl (39 kB)\n",
            "Collecting srt\n",
            "  Downloading srt-3.5.2.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting spleeter\n",
            "  Downloading spleeter-2.3.2-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from deepl) (2.27.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Collecting norbert==0.2.1\n",
            "  Downloading norbert-0.2.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tensorflow<3.0.0,>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from spleeter) (2.11.0)\n",
            "Collecting librosa<0.9.0,>=0.8.0\n",
            "  Downloading librosa-0.8.1-py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.8/203.8 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llvmlite<0.39.0,>=0.38.0\n",
            "  Downloading llvmlite-0.38.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0.0,>=3.19.4 in /usr/local/lib/python3.9/dist-packages (from spleeter) (3.19.6)\n",
            "Collecting httpx[http2]<0.20.0,>=0.19.0\n",
            "  Downloading httpx-0.19.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from spleeter) (1.4.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.2 in /usr/local/lib/python3.9/dist-packages (from spleeter) (1.22.4)\n",
            "Collecting typer<0.4.0,>=0.3.2\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from norbert==0.2.1->spleeter) (1.10.1)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.9/dist-packages (from httpx[http2]<0.20.0,>=0.19.0->spleeter) (2.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx[http2]<0.20.0,>=0.19.0->spleeter) (2022.12.7)\n",
            "Collecting httpcore<0.14.0,>=0.13.3\n",
            "  Downloading httpcore-0.13.7-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting h2<5,>=3\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (23.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (1.1.1)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (0.56.4)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (1.2.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (0.12.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa<0.9.0,>=0.8.0->spleeter) (4.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2.0,>=1.2->spleeter) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2.0,>=1.2->spleeter) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->deepl) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->deepl) (3.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (1.16.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (1.51.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (15.0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (67.6.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (0.31.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (3.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (2.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.5.0->spleeter) (23.3.3)\n",
            "Collecting click<7.2.0,>=7.1.1\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<3.0.0,>=2.5.0->spleeter) (0.40.0)\n",
            "Collecting hyperframe<7,>=6.0\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting anyio==3.*\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numba>=0.43.0\n",
            "  Downloading numba-0.56.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading numba-0.56.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-59.8.0-py3-none-any.whl (952 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.8/952.8 KB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numba>=0.43.0\n",
            "  Downloading numba-0.56.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading numba-0.55.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa<0.9.0,>=0.8.0->spleeter) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.9.0,>=0.8.0->spleeter) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.10.2->librosa<0.9.0,>=0.8.0->spleeter) (1.15.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (3.4.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (1.8.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.9.0,>=0.8.0->spleeter) (2.21)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (6.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.5.0->spleeter) (3.2.2)\n",
            "Building wheels for collected packages: srt\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.2-py3-none-any.whl size=22488 sha256=94e37453418a0286b99f74273b89b74f32ff77f1261d10a45a3e2ab38be8dca9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/cf/fc/fc66e373ba9435f65a53ea7e7aeb18e1100f2ba17b6ef17518\n",
            "Successfully built srt\n",
            "Installing collected packages: rfc3986, srt, sniffio, llvmlite, hyperframe, hpack, h11, ffmpeg-python, click, typer, numba, norbert, h2, deepl, anyio, resampy, httpcore, librosa, httpx, spleeter\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.4\n",
            "    Uninstalling numba-0.56.4:\n",
            "      Successfully uninstalled numba-0.56.4\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.0.post2\n",
            "    Uninstalling librosa-0.10.0.post2:\n",
            "      Successfully uninstalled librosa-0.10.0.post2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.3 requires click>=8.0, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyio-3.6.2 click-7.1.2 deepl-1.14.0 ffmpeg-python-0.2.0 h11-0.12.0 h2-4.1.0 hpack-4.0.0 httpcore-0.13.7 httpx-0.19.0 hyperframe-6.0.1 librosa-0.8.1 llvmlite-0.38.1 norbert-0.2.1 numba-0.55.2 resampy-0.4.2 rfc3986-1.5.0 sniffio-1.3.0 spleeter-2.3.2 srt-3.5.2 typer-0.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-zkotgpx5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-zkotgpx5\n",
            "  Resolved https://github.com/openai/whisper.git to commit 6dea21fd7f7253bfe450f1e2512a0fe47ee2d258\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tiktoken==0.3.1\n",
            "  Downloading tiktoken-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.13.1+cu116)\n",
            "Collecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230314) (0.18.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.10.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
            "Collecting lit\n",
            "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (0.38.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (67.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2.0.12)\n",
            "Building wheels for collected packages: openai-whisper, lit\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=796926 sha256=d6900b9eb8b2fc0d28e805f77310ff98cce9a90a3940a54ba2b73072f25fee0b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dentp8i1/wheels/fe/03/29/e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93601 sha256=9afff908c4f7c70f54eb3b03b3b799e6d8c3ba75ed56156b77aaa0b1e8a14d6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/ee/80/1520ca86c3557f70e5504b802072f7fc3b0e2147f376b133ed\n",
            "Successfully built openai-whisper lit\n",
            "Installing collected packages: lit, triton, tiktoken, openai-whisper\n",
            "Successfully installed lit-16.0.0 openai-whisper-20230314 tiktoken-0.3.1 triton-2.0.0\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIHjcEAMtj2m",
        "outputId": "2cea0d7c-0d1a-4d41-cc2e-fd3303d062b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "24_ZzaWLv_-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import whisper\n",
        "import os\n",
        "import ffmpeg\n",
        "import srt\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import deepl\n",
        "import urllib.request\n",
        "import json\n",
        "from google.colab import files\n",
        "import wave\n",
        "\n",
        "wav_file =  \"/content/drive/MyDrive/Y0000001036_1iqqZDqJPDA.wav\"\n",
        "audio_path = \"/content/drive/MyDrive/Y0000001036_1iqqZDqJPDA.wav\"  \n",
        "model_size = \"large\"  \n",
        "language = \"chinese\"  \n",
        "translation_mode = \"No translation\" \n",
        "deepl_authkey = \"\"  \n",
        "source_separation = False  \n",
        "vad_threshold = 0.4  \n",
        "chunk_threshold = 3.0  \n",
        "deepl_target_lang = \"ZH-CN\"  \n",
        "max_attempts = 1  \n",
        "initial_prompt = \"\"  \n",
        "count=1\n",
        "vad_chunks = '/content/vad_chunks'"
      ],
      "metadata": {
        "id": "ukrwBbHZtozv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with wave.open(wav_file, \"rb\") as wf:\n",
        "    framerate = wf.getframerate()\n",
        "    n_channels = wf.getnchannels()\n",
        "    sampwidth = wf.getsampwidth()\n",
        "    n_frames = wf.getnframes()\n",
        "    comp_type = wf.getcomptype()\n",
        "    comp_name = wf.getcompname()"
      ],
      "metadata": {
        "id": "uH5PRFRettEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation_mode = \"No translation\"\n",
        "# Configuration\n",
        "if translation_mode == \"End-to-end Whisper (default)\":\n",
        "    task = \"translate\"\n",
        "    run_deepl = False\n",
        "elif translation_mode == \"Whisper -> DeepL\":\n",
        "    task = \"transcribe\"\n",
        "    run_deepl = True\n",
        "elif translation_mode == \"No translation\":\n",
        "    task = \"transcribe\"\n",
        "    run_deepl = False\n",
        "else:\n",
        "    raise ValueError(\"Invalid translation mode\")\n",
        "if initial_prompt.strip() == \"\":\n",
        "    initial_prompt = None\n",
        "\n",
        "if \"http://\" in audio_path or \"https://\" in audio_path:\n",
        "    print(\"Downloading audio...\")\n",
        "    urllib.request.urlretrieve(audio_path, \"input_file\")\n",
        "    audio_path = \"input_file\"\n",
        "else:\n",
        "    if not os.path.exists(audio_path):\n",
        "        try:\n",
        "            audio_path = uploaded_file\n",
        "            if not os.path.exists(audio_path):\n",
        "                raise ValueError(\"Input audio not found. Is your audio_path correct?\")\n",
        "        except NameError:\n",
        "            raise ValueError(\"Input audio not found. Did you upload a file?\")"
      ],
      "metadata": {
        "id": "8uPlkYust2vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = os.path.splitext(audio_path)[0] + \".srt\"\n",
        "out_path_pre = os.path.splitext(audio_path)[0] + \"_Untranslated.srt\"\n",
        "if source_separation:\n",
        "    print(\"Separating vocals...\")\n",
        "    !ffprobe -i \"{audio_path}\" -show_entries format=duration -v quiet -of csv=\"p=0\" > input_length\n",
        "    with open(\"input_length\") as f:\n",
        "        input_length = int(float(f.read())) + 1\n",
        "    !spleeter separate -d {input_length} -p spleeter:2stems -o output \"{audio_path}\"\n",
        "    spleeter_dir = os.path.basename(os.path.splitext(audio_path)[0])\n",
        "    audio_path = \"output/\" + spleeter_dir + \"/vocals.wav\"\n",
        "\n",
        "print(\"Encoding audio...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD2yghJnt9Sc",
        "outputId": "dbe8e63f-2399-455c-e092-6194078099f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding audio...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"vad_chunks\"):\n",
        "    os.mkdir(\"vad_chunks\")"
      ],
      "metadata": {
        "id": "lMhF9gjct9-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffmpeg.input(audio_path).output(\n",
        "    \"vad_chunks/silero_temp.wav\",\n",
        "    ar=\"16000\",\n",
        "    ac=\"1\",\n",
        "    acodec=\"pcm_s16le\",\n",
        "    map_metadata=\"-1\",\n",
        "    fflags=\"+bitexact\",\n",
        ").overwrite_output().run(quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN-cKvojuBhv",
        "outputId": "f551dee3-3d21-4bb9-c836-fe51ae72c15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(b'',\n",
              " b\"ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\\n  libavutil      56. 31.100 / 56. 31.100\\n  libavcodec     58. 54.100 / 58. 54.100\\n  libavformat    58. 29.100 / 58. 29.100\\n  libavdevice    58.  8.100 / 58.  8.100\\n  libavfilter     7. 57.100 /  7. 57.100\\n  libavresample   4.  0.  0 /  4.  0.  0\\n  libswscale      5.  5.100 /  5.  5.100\\n  libswresample   3.  5.100 /  3.  5.100\\n  libpostproc    55.  5.100 / 55.  5.100\\nGuessed Channel Layout for Input Stream #0.0 : mono\\nInput #0, wav, from '/content/drive/MyDrive/Y0000001036_1iqqZDqJPDA.wav':\\n  Duration: 00:41:31.34, bitrate: 512 kb/s\\n    Stream #0:0: Audio: pcm_s32le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s32, 512 kb/s\\nStream mapping:\\n  Stream #0:0 -> #0:0 (pcm_s32le (native) -> pcm_s16le (native))\\nPress [q] to stop, [?] for help\\nOutput #0, wav, to 'vad_chunks/silero_temp.wav':\\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\\n    Metadata:\\n      encoder         : Lavc pcm_s16le\\nsize=     256kB time=00:00:08.83 bitrate= 237.5kbits/s speed=14.6x    \\rsize=   16128kB time=00:08:42.88 bitrate= 252.7kbits/s speed= 468x    \\rsize=   32000kB time=00:17:07.52 bitrate= 255.1kbits/s speed= 635x    \\rsize=   52480kB time=00:28:04.09 bitrate= 255.3kbits/s speed= 790x    \\rsize=   62976kB time=00:33:39.96 bitrate= 255.4kbits/s speed= 766x    \\rsize=   73216kB time=00:39:09.69 bitrate= 255.3kbits/s speed= 749x    \\rsize=   77855kB time=00:41:31.34 bitrate= 256.0kbits/s speed= 774x    \\nvideo:0kB audio:77854kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000055%\\n\")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_audio(file_path, start_time, end_time):\n",
        "  # 读取音频文件\n",
        "  audio = AudioSegment.from_wav(file_path)\n",
        "  # 将起始和终止时间转换为毫秒\n",
        "  start_ms = start_time * 1000\n",
        "  end_ms = end_time * 1000\n",
        "  # 切割音频文件\n",
        "  audio_segment = audio[start_ms:end_ms]\n",
        "  # 返回音频片段\n",
        "  return audio_segment\n",
        "model, utils = torch.hub.load(\n",
        "    repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", onnx=False\n",
        ")\n",
        "\n",
        "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
        "\n",
        "# Generate VAD timestamps\n",
        "VAD_SR = 16000\n",
        "wav = read_audio(\"vad_chunks/silero_temp.wav\", sampling_rate=VAD_SR)\n",
        "t = get_speech_timestamps(wav, model, sampling_rate=VAD_SR, threshold=vad_threshold)\n",
        "\n",
        "\n",
        "# Add a bit of padding, and remove small gaps\n",
        "for i in range(len(t)):\n",
        "    t[i][\"start\"] = max(0, t[i][\"start\"] - 3200)  # 0.2s head\n",
        "    t[i][\"end\"] = min(wav.shape[0] - 16, t[i][\"end\"] + 20800)  # 1.3s tail\n",
        "    if i > 0 and t[i][\"start\"] < t[i - 1][\"end\"]:\n",
        "        t[i][\"start\"] = t[i - 1][\"end\"]  # Remove overlap\n",
        "\n",
        "# If breaks are longer than chunk_threshold seconds, split into a new audio file\n",
        "# This'll effectively turn long transcriptions into many shorter ones\n",
        "u = [[]]\n",
        "for i in range(len(t)):\n",
        "    if i > 0 and t[i][\"start\"] > t[i - 1][\"end\"] + (chunk_threshold * VAD_SR):\n",
        "        u.append([])\n",
        "    u[-1].append(t[i])\n",
        "\n",
        "# Merge speech chunks\n",
        "for i in range(len(u)):\n",
        "    save_audio(\n",
        "        \"vad_chunks/\" + str(i) + \".wav\",\n",
        "        collect_chunks(u[i], wav),\n",
        "        sampling_rate=VAD_SR,\n",
        "    )\n",
        "os.remove(\"vad_chunks/silero_temp.wav\")\n",
        "\n",
        "# Convert timestamps to seconds\n",
        "for i in range(len(u)):\n",
        "    time = 0.0\n",
        "    offset = 0.0\n",
        "    for j in range(len(u[i])):\n",
        "        u[i][j][\"start\"] /= VAD_SR\n",
        "        u[i][j][\"end\"] /= VAD_SR\n",
        "        u[i][j][\"chunk_start\"] = time\n",
        "        time += u[i][j][\"end\"] - u[i][j][\"start\"]\n",
        "        u[i][j][\"chunk_end\"] = time\n",
        "        if j == 0:\n",
        "            offset += u[i][j][\"start\"]\n",
        "        else:\n",
        "            offset += u[i][j][\"start\"] - u[i][j - 1][\"end\"]\n",
        "        u[i][j][\"offset\"] = offset\n",
        "\n",
        "# Run Whisper on each audio chunk\n",
        "print(\"Running Whisper...\")\n",
        "model = whisper.load_model(model_size)\n",
        "subs = []\n",
        "segment_info = []\n",
        "sub_index = 1\n",
        "suppress_low = [\n",
        "    \"Thank you\",\n",
        "    \"Thanks for\",\n",
        "    \"ike and \",\n",
        "    \"Bye.\",\n",
        "    \"Bye!\",\n",
        "    \"Bye bye!\",\n",
        "    \"lease sub\",\n",
        "    \"The end.\",\n",
        "    \"視聴\",\n",
        "]\n",
        "suppress_high = [\n",
        "    \"ubscribe\",\n",
        "    \"my channel\",\n",
        "    \"the channel\",\n",
        "    \"our channel\",\n",
        "    \"ollow me on\",\n",
        "    \"for watching\",\n",
        "    \"hank you for watching\",\n",
        "    \"for your viewing\",\n",
        "    \"r viewing\",\n",
        "    \"Amara\",\n",
        "    \"next video\",\n",
        "    \"full video\",\n",
        "    \"ranslation by\",\n",
        "    \"ranslated by\",\n",
        "    \"ee you next week\",\n",
        "    \"ご視聴\",\n",
        "    \"視聴ありがとうございました\",\n",
        "]\n",
        "for i in tqdm(range(len(u))):\n",
        "    line_buffer = []  # Used for DeepL\n",
        "    for x in range(max_attempts):\n",
        "        result = model.transcribe(\n",
        "            \"vad_chunks/\" + str(i) + \".wav\", task=task, language=language, initial_prompt=initial_prompt\n",
        "        )\n",
        "        # Break if result doesn't end with severe hallucinations\n",
        "        if len(result[\"segments\"]) == 0:\n",
        "            break\n",
        "        elif result[\"segments\"][-1][\"end\"] < u[i][-1][\"chunk_end\"] + 10.0:\n",
        "            break\n",
        "        elif x+1 < max_attempts:\n",
        "            print(\"Retrying chunk\", i)\n",
        "    for r in result[\"segments\"]:\n",
        "        # Skip audio timestamped after the chunk has ended\n",
        "        if r[\"start\"] > u[i][-1][\"chunk_end\"]:\n",
        "            continue\n",
        "        # Reduce log probability for certain words/phrases\n",
        "        for s in suppress_low:\n",
        "            if s in r[\"text\"]:\n",
        "                r[\"avg_logprob\"] -= 0.15\n",
        "        for s in suppress_high:\n",
        "            if s in r[\"text\"]:\n",
        "                r[\"avg_logprob\"] -= 0.35\n",
        "        # Keep segment info for debugging\n",
        "        del r[\"tokens\"]\n",
        "        segment_info.append(r)\n",
        "        # Skip if log prob is low or no speech prob is high\n",
        "        if r[\"avg_logprob\"] < -1.0 or r[\"no_speech_prob\"] > 0.7:\n",
        "            continue\n",
        "        # Set start timestamp\n",
        "        start = r[\"start\"] + u[i][0][\"offset\"]\n",
        "        for j in range(len(u[i])):\n",
        "            if (\n",
        "                r[\"start\"] >= u[i][j][\"chunk_start\"]\n",
        "                and r[\"start\"] <= u[i][j][\"chunk_end\"]\n",
        "            ):\n",
        "                start = r[\"start\"] + u[i][j][\"offset\"]\n",
        "                break\n",
        "        # Prevent overlapping subs\n",
        "        if len(subs) > 0:\n",
        "            last_end = datetime.timedelta.total_seconds(subs[-1].end)\n",
        "            if last_end > start:\n",
        "                subs[-1].end = datetime.timedelta(seconds=start)\n",
        "        # Set end timestamp\n",
        "        end = u[i][-1][\"end\"] + 0.5\n",
        "        for j in range(len(u[i])):\n",
        "            if r[\"end\"] >= u[i][j][\"chunk_start\"] and r[\"end\"] <= u[i][j][\"chunk_end\"]:\n",
        "                end = r[\"end\"] + u[i][j][\"offset\"]\n",
        "                break\n",
        "        # Add to SRT list\n",
        "        subs.append(\n",
        "            srt.Subtitle(\n",
        "                index=sub_index,\n",
        "                start=datetime.timedelta(seconds=start),\n",
        "                end=datetime.timedelta(seconds=end),\n",
        "                content=r[\"text\"].strip(),\n",
        "            )\n",
        "        )\n",
        "        sub_index += 1\n",
        "    '''\n",
        "    print(result[\"segments\"])\n",
        "    for i in result[\"segments\"]:\n",
        "      start = i['start']\n",
        "      print(start)\n",
        "    '''\n",
        "    for i in result[\"segments\"]:\n",
        "      audio_segment = split_audio(audio_path, i['start'], i['end'])\n",
        "      new_file_name = f'{1}_{count}.wav'\n",
        "      new_file_path = os.path.join(vad_chunks, new_file_name)\n",
        "      audio_segment.export(new_file_path, format='wav')\n",
        "      count += 1\n",
        "\n",
        "with open(\"segment_info.json\", \"w\", encoding=\"utf8\") as f:\n",
        "    json.dump(segment_info, f, indent=4)\n",
        "\n",
        "# DeepL translation\n",
        "translate_error = False\n",
        "if run_deepl:\n",
        "    print(\"Translating...\")\n",
        "    with open(out_path_pre, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(srt.compose(subs))\n",
        "    print(\"(Untranslated subs saved to\", out_path_pre, \")\")\n",
        "\n",
        "    lines = []\n",
        "    punct_match = [\"。\", \"、\", \",\", \".\", \"〜\", \"！\", \"!\", \"？\", \"?\", \"-\"]\n",
        "    for i in range(len(subs)):\n",
        "        if language.lower() == \"japanese\":\n",
        "            if subs[i].content[-1] not in punct_match:\n",
        "                subs[i].content += \"。\"\n",
        "            subs[i].content = \"「\" + subs[i].content + \"」\"\n",
        "        else:\n",
        "            if subs[i].content[-1] not in punct_match:\n",
        "                subs[i].content += \".\"\n",
        "            subs[i].content = '\"' + subs[i].content + '\"'\n",
        "    for i in range(len(subs)):\n",
        "        lines.append(subs[i].content)\n",
        "\n",
        "    grouped_lines = []\n",
        "    english_lines = []\n",
        "    for i, l in enumerate(lines):\n",
        "        if i % 30 == 0:\n",
        "            # Split lines into smaller groups, to prevent error 413\n",
        "            grouped_lines.append([])\n",
        "            if i != 0:\n",
        "                # Include previous 3 lines, to preserve context between splits\n",
        "                grouped_lines[-1].extend(grouped_lines[-2][-3:])\n",
        "        grouped_lines[-1].append(l.strip())\n",
        "        \n",
        "    try:\n",
        "        translator = deepl.Translator(deepl_authkey)\n",
        "        for i, n in enumerate(tqdm(grouped_lines)):\n",
        "            x = [\"\\n\".join(n).strip()]\n",
        "            if language.lower() == \"japanese\":\n",
        "                result = translator.translate_text(x, source_lang=\"JA\", target_lang=deepl_target_lang)\n",
        "            else:\n",
        "                result = translator.translate_text(x, target_lang=deepl_target_lang)\n",
        "            english_tl = result[0].text.strip().splitlines()\n",
        "            assert len(english_tl) == len(n), (\n",
        "                \"Invalid translation line count (\"\n",
        "                + str(len(english_tl))\n",
        "                + \" vs \"\n",
        "                + str(len(n))\n",
        "                + \")\"\n",
        "            )\n",
        "            if i != 0:\n",
        "                english_tl = english_tl[3:]\n",
        "            remove_quotes = dict.fromkeys(map(ord, '\"„“‟”＂「」'), None)\n",
        "            for e in english_tl:\n",
        "                english_lines.append(\n",
        "                    e.strip().translate(remove_quotes).replace(\"’\", \"'\")\n",
        "                )\n",
        "        for i, e in enumerate(english_lines):\n",
        "            subs[i].content = e\n",
        "    except Exception as e:\n",
        "        print(\"DeepL translation error:\", e)\n",
        "        print(\"(downloading untranslated version instead)\")\n",
        "        translate_error = True\n",
        "\n",
        "# Write SRT file\n",
        "if translate_error:\n",
        "    files.download(out_path_pre)\n",
        "else:\n",
        "    # Removal of garbage lines\n",
        "    garbage_list = [\n",
        "        \"a\",\n",
        "        \"aa\",\n",
        "        \"ah\",\n",
        "        \"ahh\",\n",
        "        \"ha\",\n",
        "        \"haa\",\n",
        "        \"hah\",\n",
        "        \"haha\",\n",
        "        \"hahaha\",\n",
        "        \"mmm\",\n",
        "        \"mm\",\n",
        "        \"m\",\n",
        "        \"h\",\n",
        "        \"o\",\n",
        "        \"mh\",\n",
        "        \"mmh\",\n",
        "        \"hm\",\n",
        "        \"hmm\",\n",
        "        \"huh\",\n",
        "        \"oh\",\n",
        "    ]\n",
        "    need_context_lines = [\n",
        "        \"feelsgod\",\n",
        "        \"godbye\",\n",
        "        \"godnight\",\n",
        "        \"thankyou\",\n",
        "    ]\n",
        "    clean_subs = list()\n",
        "    last_line_garbage = False\n",
        "    for i in range(len(subs)):\n",
        "        c = subs[i].content\n",
        "        c = (\n",
        "            c.replace(\".\", \"\")\n",
        "            .replace(\",\", \"\")\n",
        "            .replace(\":\", \"\")\n",
        "            .replace(\";\", \"\")\n",
        "            .replace(\"!\", \"\")\n",
        "            .replace(\"?\", \"\")\n",
        "            .replace(\"-\", \" \")\n",
        "            .replace(\"  \", \" \")\n",
        "            .replace(\"  \", \" \")\n",
        "            .replace(\"  \", \" \")\n",
        "            .lower()\n",
        "            .replace(\"that feels\", \"feels\")\n",
        "            .replace(\"it feels\", \"feels\")\n",
        "            .replace(\"feels good\", \"feelsgood\")\n",
        "            .replace(\"good bye\", \"goodbye\")\n",
        "            .replace(\"good night\", \"goodnight\")\n",
        "            .replace(\"thank you\", \"thankyou\")\n",
        "            .replace(\"aaaaaa\", \"a\")\n",
        "            .replace(\"aaaa\", \"a\")\n",
        "            .replace(\"aa\", \"a\")\n",
        "            .replace(\"aa\", \"a\")\n",
        "            .replace(\"mmmmmm\", \"m\")\n",
        "            .replace(\"mmmm\", \"m\")\n",
        "            .replace(\"mm\", \"m\")\n",
        "            .replace(\"mm\", \"m\")\n",
        "            .replace(\"hhhhhh\", \"h\")\n",
        "            .replace(\"hhhh\", \"h\")\n",
        "            .replace(\"hh\", \"h\")\n",
        "            .replace(\"hh\", \"h\")\n",
        "            .replace(\"oooooo\", \"o\")\n",
        "            .replace(\"oooo\", \"o\")\n",
        "            .replace(\"oo\", \"o\")\n",
        "            .replace(\"oo\", \"o\")\n",
        "        )\n",
        "        is_garbage = True\n",
        "        for w in c.split(\" \"):\n",
        "            if w.strip() == \"\":\n",
        "                continue\n",
        "            if w.strip() in garbage_list:\n",
        "                continue\n",
        "            elif w.strip() in need_context_lines and last_line_garbage:\n",
        "                continue\n",
        "            else:\n",
        "                is_garbage = False\n",
        "                break\n",
        "        if not is_garbage:\n",
        "            clean_subs.append(subs[i])\n",
        "        last_line_garbage = is_garbage\n",
        "    with open(out_path, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(srt.compose(clean_subs))\n",
        "    print(\"\\nDone! Subs written to\", out_path)\n",
        "    print(\"Downloading SRT file:\")\n",
        "    files.download(out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FUvZmkdnlUIA",
        "outputId": "c8926155-0aa9-42ef-f19f-90132aa90805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/56 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "  2%|▏         | 1/56 [06:20<5:48:44, 380.45s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "  4%|▎         | 2/56 [07:34<3:00:16, 200.31s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "  5%|▌         | 3/56 [08:50<2:06:54, 143.68s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "  7%|▋         | 4/56 [10:11<1:42:55, 118.76s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "  9%|▉         | 5/56 [16:19<2:57:12, 208.49s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 11%|█         | 6/56 [23:01<3:48:40, 274.41s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 12%|█▎        | 7/56 [24:45<2:58:28, 218.55s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 14%|█▍        | 8/56 [25:42<2:13:44, 167.18s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 16%|█▌        | 9/56 [26:43<1:45:00, 134.06s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 18%|█▊        | 10/56 [32:26<2:32:17, 198.64s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 20%|█▉        | 11/56 [39:00<3:13:45, 258.34s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 21%|██▏       | 12/56 [40:08<2:26:58, 200.42s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 23%|██▎       | 13/56 [41:17<1:55:12, 160.76s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 25%|██▌       | 14/56 [43:16<1:43:34, 147.97s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 27%|██▋       | 15/56 [1:00:53<4:48:21, 421.98s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 29%|██▊       | 16/56 [1:02:48<3:39:51, 329.79s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 30%|███       | 17/56 [1:04:48<2:53:15, 266.55s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 32%|███▏      | 18/56 [1:09:44<2:54:27, 275.47s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 34%|███▍      | 19/56 [1:13:25<2:39:49, 259.18s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 36%|███▌      | 20/56 [1:15:39<2:12:55, 221.54s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 38%|███▊      | 21/56 [1:21:31<2:32:01, 260.63s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 39%|███▉      | 22/56 [1:23:43<2:05:51, 222.09s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 41%|████      | 23/56 [1:25:07<1:39:20, 180.62s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 43%|████▎     | 24/56 [1:26:07<1:17:00, 144.38s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 45%|████▍     | 25/56 [1:27:23<1:04:03, 123.98s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 46%|████▋     | 26/56 [1:28:19<51:49, 103.64s/it]  /usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 48%|████▊     | 27/56 [1:33:54<1:23:31, 172.80s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 50%|█████     | 28/56 [1:39:01<1:39:27, 213.12s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 52%|█████▏    | 29/56 [1:41:26<1:26:42, 192.67s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 54%|█████▎    | 30/56 [1:42:51<1:09:29, 160.38s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 55%|█████▌    | 31/56 [1:44:03<55:50, 134.02s/it]  /usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 57%|█████▋    | 32/56 [1:45:44<49:32, 123.87s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 59%|█████▉    | 33/56 [1:48:53<55:02, 143.60s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 61%|██████    | 34/56 [1:52:14<58:53, 160.63s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 62%|██████▎   | 35/56 [1:55:53<1:02:23, 178.27s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 64%|██████▍   | 36/56 [1:57:23<50:34, 151.74s/it]  /usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 66%|██████▌   | 37/56 [1:58:37<40:38, 128.37s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 68%|██████▊   | 38/56 [2:00:04<34:48, 116.04s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 70%|██████▉   | 39/56 [2:01:27<30:06, 106.24s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 71%|███████▏  | 40/56 [2:05:10<37:38, 141.18s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 73%|███████▎  | 41/56 [2:10:49<50:08, 200.58s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 75%|███████▌  | 42/56 [2:11:59<37:38, 161.33s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 77%|███████▋  | 43/56 [2:12:57<28:16, 130.49s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 79%|███████▊  | 44/56 [2:23:33<56:23, 281.97s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 80%|████████  | 45/56 [2:29:13<54:54, 299.49s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 82%|████████▏ | 46/56 [2:30:17<38:08, 228.83s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 84%|████████▍ | 47/56 [2:31:24<27:02, 180.24s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 86%|████████▌ | 48/56 [2:36:59<30:13, 226.66s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 88%|████████▊ | 49/56 [2:50:36<47:07, 403.89s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 89%|████████▉ | 50/56 [2:51:43<30:15, 302.63s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 91%|█████████ | 51/56 [2:52:59<19:33, 234.74s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 93%|█████████▎| 52/56 [3:07:02<27:48, 417.11s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 95%|█████████▍| 53/56 [3:11:01<18:11, 363.74s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 96%|█████████▋| 54/56 [3:12:37<09:26, 283.45s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            " 98%|█████████▊| 55/56 [3:14:28<03:51, 231.54s/it]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 56/56 [3:26:43<00:00, 221.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done! Subs written to /content/drive/MyDrive/Y0000001036_1iqqZDqJPDA.srt\n",
            "Downloading SRT file:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_14c7bf06-1aa0-4cff-966c-8a0a5ad52065\", \"Y0000001036_1iqqZDqJPDA.srt\", 44578)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}